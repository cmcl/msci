\section{Formalisation}

The formalisation was performed using the Coq proof
assisstant~\cite{Coq:manual}. In the spirit of the POPLMARK
challenge~\cite{Aydemir:2005:MMM}, the development utilises available
libraries and infrastructure as much as possible. In particular,
\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM} developed a metatheory
library for handling, among other things, association lists (environments) and
free variable representation. The current development uses the version of the
library distributed by~\citeauthor{Park:2014:MMW}~\cite{Park:2014:MMW} which
has been updated for Coq version 8.4pl4. One goal of the formalisation effort
was to minimise the extent to which the development differs from the
pen-and-paper presentation and avoid questions of adequacy of the
representation. Some changes were made to ease the mechanisation and are
highlighted where they occur.

\subsection{Preliminaries}\label{sec:approach}

%\input{with-connect-rule}
\input{cp}

The locally nameless first-order representation for binders and free variables
was adopted for both GV and CP, following
\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM}. This first-order
variant prevents variable-capture issues when compared with other first-order
approaches. Alternatively, I could have chosen a higher-order abstract syntax
(HOAS) approach such as the parametric version (PHOAS) described
by~\citeauthor{Chlipala:2008:PHOAS}~\cite{Chlipala:2008:PHOAS}. PHOAS relies
on the metalanguage (Coq, in this case) for handling binders and substitution
so has less overhead in managing syntax than the locally nameless
approach. However, it is less clear how to handle linearity using this
approach and has the disadvantage of being able to represent terms not valid
in the language being mechanised (so called ``exotic'' terms). Further, a
higher-order approach would prevent the use of the Metatheory library which
provides a number of useful tactics for manipulating typing contexts.

Previous efforts to mechanise \fpop~\cite{Park:2014:MMW} illustrate how to
handle a language containing both linear and non-linear contexts. However, to
stay true to CP/GV, I maintain a single context. Interestingly, it became
clear that the mechanised version of CP required explicit weakening and
contraction to facilitate the reduction rules (\S~\ref{sec:cp}).

For handling binder freshness, I chose to adopt the cofinite quantification
approach described by~\citeauthor{Aydemir:2008:EFM}, which excludes a finite
set of variables from being considered as the binder. In contrast, the
traditional ``exists-fresh'' approach, where the binder is only required to be
fresh within the abstraction's body, does not produce strong enough induction
principles in some cases. This change will necessitate changes to the typing
rules of GV and CP. Consider for instance the Connect rule in GV (C) and its
corresponding cofinite quantification version (CCO) in
Figure~\ref{fig:connect}. The conclusion specifies a variable in (C) but a
type in (CCO) (de Bruijn index rather than a name). Thus, in the premise for
(CCO) one can choose a suitably fresh $x$ in each premise. Note that, $L$ can
be chosen such that $\Psi \cup \Phi \cup \fv(M) \cup \fv(N) \subseteq L$,
where $\fv(M)$ are the set of free variables in term $M$, and $x$ can be the
same in both premises. Similar changes are made to the rules for CP. These
changes do not alter the typing of processes nor the semantics of the
translation from GV to CP. An equivalence between ``exists-fresh'' and
cofinite quantification definitions of the simply-typed $\lambda$-calculus is
given by \citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM}, and I suspect
a similar result could be derived straightforwardly (but tediously) for the
CP/GV system.

Another aspect of the encoding is how to define terms. The intrinsic encoding
described by~\citeauthor{Benton:2012:STT}~\cite{Benton:2012:STT} is one
approach, indexing terms by their type so as to prevent ill-formed terms from
being constructed. However, in the encoding for CP/GV there are issues with
using this encoding: $(1)$ it is not immediately clear how to handle linear
contexts using the de Bruijn variable encoding, since in the intrinsic setting
the environment automatically supports weakening; $(2)$ an intrinsic encoding
of GV terms would be complicated by the need to enforce that session types
occur in certain instances (by use of a predicate); and $(3)$ well-typed terms
require extra assumptions about binder freshness which cannot reasonably be
expressed as a function type. For these reasons, an intrinsic approach does
not offer much benefit since a well-typed term relation would still need to be
defined. Therefore, I chose an extrinsic encoding for terms and define
well-typed terms as a separate inductive type.

\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM} note that the size of
their language proof infrastructure is proportional to the number of binding
constructs. In the case of a simply typed lambda calculus this is not onerous,
but GV has four binding constructs, CP has six and propositions have
two. Indeed, a lot of effort was expended setting up this
infrastructure. However, I believe one of the contributions of this work is to
articulate what one needs from state of the art metatheory libraries in order
to handle session-based languages, and more generally, linear type systems.

\citeauthor{Park:2014:MMW} describe a technique for
removing nonlinear contexts from typing judgements. While their work extended
to \fpop it is not clear how to handle an environment containing both
non-linear and linear types as in GV. I wish to maintain as close a
relationship as possible to the paper system presented by
\citeauthor{Wadler:2014}, so separating out the non-linear and linear
components (as in \fpop) is not an option at this stage.

\begin{comment}
\section{Issues}

\begin{itemize}
\item What is required of a Metatheory library for linear type systems?
  Permutation reasoning? Weakening/Contraction for non-linear components?
\item Discuss the different approaches that were taken to maintain close
  correspondence with the paper presentation
\item Make sure to note that the paper leaves details (a lot of the ``cruft'')
  out of the proofs e.g. where weakening is applied in the typing derivations
\item Mention the use of Metatheory and its limitations when mechanising
  linear type systems like GV/CP
\end{itemize}
\end{comment}

\subsection{The process calculus CP}\label{sec:cp}

\begin{comment}
FIGURES

propositions

processes

typing judgements

equivalences and axiomatic cut

principal cut rules

commuting conversions
\end{comment}

Formalising the process calculus CP provided many challenges for the
metatheory library since to my knowledge, the library has never been applied
to a process calculus before. So for instance, in the lambda calculus one
typically does not work up to permutation of binders, whereas in a process
calculus these kinds of structural equivalences are standard. Additionally,
substitution and opening are only defined on processes for names. In other
words, one cannot substitute an arbitrary process term for a name, unlike in
the lambda calculus. Thus frameworks which assume a lambda calculi style
substitution operation are not immediately amenable to the CP
setting~\cite{Lee:2012}.

Note that the rules for polymorphism, $\exists$ and $\forall$, are defined but
cut elimination on them is not. Since GV (\S~\ref{sec:gv}) does not support
polymorphism the reduction rules are unnecessary for the translation. Future
work is to support reduction under these terms.

Initially, an implicit notion of weakening and contraction were chosen so as
to be in line with \citeauthor{Wadler:2014}'s definitions. However, explicit
weakening and contraction were required to differentiate the reduction rules
for the various interactions between servers and clients.

Ordering in environments is ignored and the development required lemmas
allowing the permutation of environments. Support for permutating environments
is not currently supported in the Metatheory library but some general lemmas
were added as a result of this work. Additionally, custom tactics were
developed to automatatically solve some simple permutation goals. However,
these tactics are not a ``catch all'' and some generalising still needs to be
done.

The paper presentation omits details: the principal cut rules do not specify
weakening could occur: since weakening is implicit is tacitly assumed
weakening can always be pushed further up the derivation tree e.g. AxCut rule

another interesting aspect which the formalism reveal is the rule for
associativity is constructed in a way to permit associativity in a certain way
pre-determined

metatheory library and the library of tactics from software foundations are
used extensively: metatheory does not handle multiple binders (check this?)
maybe add a generic tactic to handle multiple binders... or perhaps add a
lemmas for handling binders

\subsection{The functional language GV}\label{sec:gv}

\begin{comment}
FIGURES

types

terms

typing judgements
\end{comment}

While the aim is to be faithful to \citeauthor{Wadler:2014}'s description, I
may need to alter the approach slightly to avoid tricky formulations. For
instance, instead of providing $n$-ary branch and choice in GV, I provide
only binary versions of these operators to simplify the development. This
change is not restrictive however, since CP itself provides only binary
versions of plus ($\oplus$) and with ($\with$) constructs. Likewise, on a more
technical note, I amalgamate all types into one inductive definition and
provide a predicate for restricting typing rules to consider only valid
session types. \citeauthor{Wadler:2014} defines GV types as a set of mutually
recursive definitions; session types are types which may contain types as
subcomponents (arguments to send, for example). Unfortunately, handling
mutually inductive definitions in Coq can be quite involved; requiring one to
either rely on the Coq system to provide a stronger mutual induction principle
or defining one manually. Such a definition is possible, but it complicates
elimination via the induction principle.

\subsection{CPS translation GV to CP}

Currently the formalisation does not handle this. Future work.
