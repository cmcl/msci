\section{Formalisation}

The formalisation was performed using the Coq proof
assistant~\cite{Coq:manual}. In the spirit of the POPLMARK
challenge~\cite{Aydemir:2005:MMM}, the development utilises available
libraries and infrastructure as much as possible. In particular,
\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM} developed a metatheory
library for handling, among other things, association lists (environments) and
variable representation. The current development uses the version of the
library distributed by~\citeauthor{Park:2014:MMW}~\cite{Park:2014:MMW} which
has been updated for Coq version 8.4pl4. One goal of the formalisation effort
was to minimise the extent to which the development differs from the
pen-and-paper presentation and avoid questions of adequacy of the
representation. Some changes were made to ease the mechanisation and are
highlighted where they occur.

\subsection{Preliminaries}\label{sec:approach}

The locally nameless cofinite quantification (LNCQ) representation for binders
and free variables was adopted for both GV and CP, following
\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM}. This first-order
variant prevents variable-capture issues when compared with other first-order
approaches. Alternatively, I could have chosen a higher-order abstract syntax
(HOAS) approach such as the parametric version (PHOAS) described
by~\citeauthor{Chlipala:2008:PHOAS}~\cite{Chlipala:2008:PHOAS}. PHOAS relies
on the metalanguage (Coq, in this case) for handling binders and substitution
so has less overhead in managing syntax than the locally nameless
approach. However, it is less clear how to handle linearity using this
approach and has the disadvantage of being able to represent terms not valid
in the language being mechanised (so called ``exotic'' terms). Further, a
higher-order approach would prevent the use of the Metatheory library which
provides a number of useful tactics for manipulating typing contexts.

Previous efforts to mechanise \fpop~\cite{Park:2014:MMW} illustrate how to
handle a language containing both linear and non-linear contexts. However, to
stay true to CP/GV, I maintain a single context. Interestingly, it became
clear that the mechanised version of CP required explicit weakening and
contraction to facilitate the reduction rules (\S~\ref{sec:cp}).

For handling binder freshness, I chose to adopt the cofinite quantification
approach described by~\citeauthor{Aydemir:2008:EFM}, which excludes a finite
set of variables from being considered as the binder. In contrast, the
traditional ``exists-fresh'' approach, where the binder is only required to be
fresh within the abstraction's body, does not produce strong enough induction
principles in some cases, e.g. weakening~\cite{Aydemir:2008:EFM}. This
decision impacts upon the premises of the CP/GV typing rules (\S~\ref{sec:cp}
and \S~\ref{sec:gv}), however these changes do not alter the typing of
processes nor the semantics of the translation from GV to CP. An equivalence
between ``exists-fresh'' and cofinite quantification definitions for the
simply-typed $\lambda$-calculus ($\stlc$) is given by
\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM}, and I suspect a similar
result could be derived straightforwardly (but tediously) for the CP/GV
system.

Another aspect of the encoding is how to define terms. The intrinsic encoding
described by~\citeauthor{Benton:2012:STT}~\cite{Benton:2012:STT} is one
approach, indexing terms by their type so as to prevent ill-formed terms from
being constructed. However, there are issues with using this encoding for
CP/GV:
\begin{enumerate}
\item It is not immediately clear how to handle linear contexts using the de
  Bruijn variable encoding, since in the intrinsic setting the environment
  automatically supports weakening
\item An intrinsic encoding of GV terms would be complicated by the need to
  enforce that session types occur in certain instances (by use of a
  predicate)
\item Well-typed terms require extra assumptions about binder freshness which
  cannot reasonably be expressed as a function type.
\end{enumerate}

For these reasons, an intrinsic approach does not offer much benefit since a
well-typed term relation would still need to be defined. Therefore, I chose an
extrinsic encoding for terms and define well-typed terms as a separate
inductive type.

\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM} note that the size of
their language proof infrastructure is proportional to the number of binding
constructs. In the case of a $\stlc$ this is not onerous, but GV has four
binding constructs, CP has six and propositions have two. Indeed, a lot of
effort was expended setting up this infrastructure. However, I believe one of
the contributions of this work is to articulate what one needs from state of
the art metatheory libraries in order to handle session-based languages, and
more generally, linear type systems.

\citeauthor{Park:2014:MMW} describe a technique for removing non-linear
contexts from typing judgements. While their work extended to \fpop it is not
clear how to handle an environment containing both non-linear and linear types
as in CP/GV. I wish to maintain as close a relationship as possible to the
pen-and-paper system presented by \citeauthor{Wadler:2014}, so separating out
the non-linear and linear components (as in \fpop) is not an option at this
stage.

\begin{comment}
\section{Issues}

\begin{itemize}
\item What is required of a Metatheory library for linear type systems?
  Permutation reasoning? Weakening/Contraction for non-linear components?
\item Discuss the different approaches that were taken to maintain close
  correspondence with the paper presentation
\item Make sure to note that the paper leaves details (a lot of the ``cruft'')
  out of the proofs e.g. where weakening is applied in the typing derivations
\item Mention the use of Metatheory and its limitations when mechanising
  linear type systems like GV/CP
\end{itemize}
\end{comment}

\subsection{The process calculus CP}\label{sec:cp}

\input{cp}

\begin{comment}
FIGURES

equivalences and axiomatic cut
\end{comment}

% Description of the typing rules...

Figure~\ref{fig:cll} presents the process calculus CP in LNCQ form with some
tedious details relating to environment uniqueness and ordering (see below)
elided. In other words, bound variables are represented using de Brujin
indices and free variables are represented using an abstract type provided by
the Metatheory library. The rules with binders reflect this by specifying the
type of a bound variable in the constructor rather than a name. Cofinite
quantification is performed in the hypotheses to these rules by excluding a
finite set of names from opening the subprocesses where opening a process $P$
with name $x$ is denoted $P^x$. Note that the rules for polymorphism,
$\exists$ and $\forall$, were not included in the final Coq development since
GV (\S~\ref{sec:gv}) does not support polymorphism making the rules
unnecessary for the translation. Further, as noted by \citeauthor{Lee:2012},
support for polymorphism in type variables creates an explosion of the
infrastructure requiring additional of opening and substituting definitions
and lemmas. They appear in the Figure simply for the sake of completeness.

Formalising the process calculus CP provided many challenges for the
Metatheory library since to my knowledge, the library has never been applied
to a process calculus before, and the treatment of names, binding and
substitution differ to that present in $\lambda$-calculi. For instance, in the
$\stlc$ one typically does not work up to permutation of binders, whereas in a
process calculus these kinds of structural equivalences are
standard. Additionally, substitution and opening are only defined for
names. In other words, one cannot substitute an arbitrary process term for a
name, unlike in the $\lambda$-calculus. Thus frameworks which assume
$\lambda$-calculi style operations are not immediately amenable to the CP
setting~\cite{Lee:2012}.

Ordering in environments is ignored so the development required operations for
handling permutations of environments. The Metatheory library does not
currently support permutating environments, however some basic lemmas were
added as a result of this work, and custom tactics were developed to
automatically solve certain permutation goals via simple proof search. Even
so, there are still many cases where sequences of primitive lemmas, such as
those handling append, commutativity and transitivity, need to be applied
manually. A lot of these cases are similar but there would not appear to be an
advantage in developing a tactic to do the work. Doing so would create a
dependency between a hard-to-debug and obscure Ltac definition and the proofs
which utilise it, becoming a burden to change. Further, such a development
philosophy treats Ltac tactics akin to macros, a shorthand for expressing an
oft-repeated sequence of tactics, and it is doubtful much can be gained from
it unless the execution of Ltac tactics can be made more transparent.

\input{principalcp}

The principal cut elimination rules are presented in Figure~\ref{??}. The
reduction rules were complicated to prove due to being composed of multiple
constructor instances and differ significantly to the standard operational
semantics one typically deals with in $\lambda$-calculus formalisms. As a
result, these well-typed reductions were not proved ``in-place'', instead each
rule is proved as a separate lemma. Most proofs followed a pattern but it was
difficult to automate satisfactorily for the reasons stated in the previous
paragraph. However, some tasks could be delegated to a custom tactic, for
example instantiating cofinitely quantified hypotheses with a newly introduced
fresh variable.

Initially, an implicit notion of weakening was chosen so as to be in line with
\citeauthor{Wadler:2014}'s definitions. However, explicit weakening is
required to differentiate the principal cut elimination rules for the various
interactions between servers and clients ($\beta_{!?}$ and $\beta_{!W}$). The
paper presentation does not specify where weakening could occur; it is tacitly
assumed weakening can always be pushed further up the derivation tree to a
base rule e.g. $\Ax\Cut$ rule. Contraction is not supported in the
formalisation due to the difficulty in specifying its principal cut
elimination rule. In order to specify the rule one requires to show the
following:
\begin{gather*}
\tm{\nu A.(!0(A).P \mid ?z[[A]].Q)}
\quad \becomes \quad \\
\tm{\nu A.(!0(A).P \mid \nu A.(!0(A).P \mid \key{contract}_{\Gamma} Q))}
\end{gather*}

where $\tm{?z[[A]].Q}$ denotes contraction by $z$ of two channels of type $A$
in $Q$, $\tm{P^y} \tpvdash \tp{{?\Gamma} \comma \tmof{y}A}$ for some
sufficiently fresh name $y$ and $\key{contract}_{\Gamma} \tm{Q}$ contracts
$\tm{Q}$ with the names in $\Gamma$.

Unfortunately, the reduction relation does not have knowledge of the
environment, $\Gamma$, typing process $\tm{P}$. This fact does not impact upon
expressing weakening because only the names (not the types in the bindings)
within the environment are required for weakening. one can compare alms with this version of cp
since it weakening rule corresponds to affine logic

In the empty choice rule there may only be a single binding in the context
(cf. an arbitrary context in \citeauthor{Wadler:2014}'s
presentation~\cite{Wadler:2014}). While this diverges with the presentation of
linear logic it became necessary to do this because otherwise there would be
no relationship between the free variables within a process and its
environment. no there is a clear iff relationship which permits reduction
rules to apply such as i think weakening but it should be any rule which takes
some fvs of proc and infers something about environments from that...otherwise
what it is not at all clear how to encode the reductions in a relation, the
subprocesses environments are created from destructing the environment

% ...commuting conversions

\input{cpstructeq}

\input{commconvonecp}

\input{commconvtwocp}

The structural equivalences and commuting conversions are shown in
Figure~\ref{??} and Figures~\ref{??} and~\ref{??}, respectively. These rules
are expressed to indicate some additional complexity not present in the
original presentation. For example, structural equivalences are not unique and
different process terms result depending on initial binder
permutation. $\permb{0}{1}$ indicates the permuting of binders $0$ and $1$ in
process $\tm{P}$. The paper presentation specifies only one possible
associativity rule lending to the specific split of the linear environments.
Notice that permutating binders is essential to formulating the majority of
these rules. Analysing permutations containing singleton bindings was
necessary for the commuting conversion rules. Singleton bindings can occur
frequently during a proof and it is sometimes necessary to analyse a
permutation of the form P++x~A ~~ Q++R to determine whether x occurs in Q or
R. the rule for associativity requires this and so do quite a few reduction
rules. since this involves some element of searching the context followed by
destructing hypotheses it could be effectively automated in permutations in
subprocesses which use a binder of the parent. the rule for associativity w
linear environments caused a problem with the splitting up of variables.

% ... subject reduction ...

subject reduction proof proceeds by induction on the reduction relation then
is automatic by appeal to the proved well-typed reductions.

% ... cut elimination ....

more elaborate than first thought. complicated measure.  as it stands the
current definitions of well-typed terms and reduction are not amenable to the
proving of the cut elimination theorem. some method of measure is necessary in
order to develop some notion of termination and also some relationship to the
the the underlying subprocesses additionally there is also the issue of the
representation which includes cases that are not immediately clear how to
prove for instance the commuting conversions are one such example the
reduction is only defined if the logical operator corresponds to a free
variable but induction on the typing judgement leads to two cases one where
the logical operator is applied to the top-level cut variable it is not
immediately clear how to eliminate this from happening a relationship between
the two subprocesses of the cut would need to be stated to prevent them from
differing so that a principal cut reduction rule may be applied yes only then
may the the rule be reduced to a cut-free thing if we had a relationship
between them then there would be some kind of thing that would allow
destruction on one then fixing the other in the case of a bvar equal to zero
\todo{how to proceed?}

\subsection{The functional language GV}\label{sec:gv}

\input{gv}

The typing judgements for the functional language GV are shown in
Figure~\ref{??}. Note that I have altered the $n$-ary branch and choice
constructs to the restricted binary versions in order to avoid tricky
formulations. The change is not restrictive however, since CP itself provides
only binary versions of plus ($\oplus$) and with ($\with$)
constructs. Explicit limiting and unlimiting of abstraction was chosen to
enable the inclusion of these rules in the translation from GV to CP, which is
performed by induction on GV terms.

On a more technical note, all types are amalgamated into one inductive
definition and provide a predicate for restricting typing rules to consider
only valid session types. \citeauthor{Wadler:2014} defines GV types as a set
of mutually recursive definitions; session types are types which may contain
types as subcomponents (arguments to send, for example). Unfortunately,
handling mutually inductive definitions in Coq can be quite involved;
requiring one to either rely on the Coq system to provide a stronger mutual
induction principle or defining one manually. Such a definition is possible,
but it complicates applications of the induction principle.

Another issue related to types is how to representing type kinds (linearity or
non-linearity). Initially the type definition was defined as an indexed
family, indexed by the kind. While this definition has the advantage of
preventing certain ill-formed types, such as a non-linear session type, it
complicated later definitions and proofs. For instance, the decidability of
equality on types now relied on an axiom permitting heterogeneous
equality~\cite{??}. Generally, it is better to avoid unnecessary postulates if
possible. Further, when formulating the typing judgements the kind must be
given to each type hypothesised and most rules (e.g. ...) are polymorphic in
the kind, motivating an awkward inspection function to be applied per
application of a constructor \todo{clean this description up}. A solution to
this was to define a well-formed type relation and do away with the type
dependency. these issues highlighted that one should be careful and
considerate regarding when to use dependent types. these issues highlighted
that one should be careful and considerate how the representation will affect
later expressiveness.

similar infrastructure for gv required as for cp copy and paste job

\subsection{CPS translation GV to CP}

require type inference on gv terms to construct corresponding cp terms but we
are assured that this is okay since the theorem only provides a well-typed gv
term in a context that is well-defined

type inference of gv terms required cluttered definition of translation with
impossible error cases. avoided a dependently typed solution due to
difficulties encountered earlier in the development. james mckinna et
al. suggest encoding the successful cases in an inductive type and then
defining the function to act on these cases. will this really work? this means
a graph of positive cases for types then a function definition for it too this
seems rather pathetic because i have to define th einference on terms and it
seems wrong i have then to handle environments but also nonenvironments i
can't remember what they are called unbound variables that is it they have to
be handled as well so we have all this clutter and the types too that is the
important part but the environments then have to be handled linearly
unfortunately very bad because we could just use the well typed term relation
for most of this headachey breaky stuff i'm not sure about that but what is
can i say so for example lets see see no i also have to thing about the
general guidelines i can be giving for this section so we have something to
say about guidelines do we yes of course we have can say that out of this we
realised that it is important for a representation to be good enough to
succintly express that which we wish to say in particular the graph relation
the inductive type that we define propositionally is better one would think
than the other way the monadic possible failure way is not producing a valid
thing that we can inductively induct on so we concern ourself with the proof
of translation preserving the typed term and i think it is important to
remember the problem we had with the inductive hypothesis we had something do
with translating the inner term of a weakening term in a larger context than
the one we had available and that was to do with the destruction of the term
inference thing i think no no it was something else we had the inner term and
we had it well typed in an environment that was smaller sure and then our
thing our function was saying we had some environment that typed both the
inner term and the outer weakening term but we know that is wrong so wrong so
here is perhaps where the inductive type graph relation thing wins out in that
we can have a hypothesis for the inner term which is sufficiently small for
the environment and we pattern match not only on the term but also on the
environment yes that is we will do because it seems to work only when we get
to the messy two the messy two constructor caases will this become a bigg
problem inthat we will then have all this stuff about environment being split
and needing to be recombined in the composite process that is a term not a
process but the point still stands it will need to be about the term and the
splitting is really a duplication of the unfortunately the well typed term
relation but otherwise i cannot think how to reuse or obtain that information
for the translation to cp

Translation uses the coq-ext-lib from michela for monadic notation

Currently the formalisation does not handle this. Future work.

\todo{how might we make progress on this?}

\subsection{Issues and Guidelines}

modularised development; tactics slow prover took a long time to reduce proofs
so split the structure of the development \todo{picture of development
  structure}.

tactic performance is an issue to be addressed due to the slow proof search a
future work (things like deciding equations on finite sets)

One of the design goals of the development was to re-use existing libraries
wherever possible. two such instances proved extremely useful. first is the
Metatheory library developed at upenn. second, also from upenn, is the library
of tactics from software foundations.

software foundations helped in many ways: simplifying tacticals, ease of
introducing proved assertions, etc. limited use of names introduced by prover
leading to more robust development.

used extensively: metatheory does not handle multiple binders (check this?)
maybe add a generic tactic to handle multiple binders... or perhaps add a
lemmas for handling binders

Linearity create issues with typing some examples; the splitting of the
environment was non-deterministic so required side-conditions to enforce a
certain splitting of binders. For example the rule for associativity of
cut. \todo{how to handle? is there a better way?}

Base rules and rules with binders required permutation assumptions to allow
any ordering of the constituent contexts. a lot of the proofs for well-typed
reductions involved small amounts of permuting environments which were largely
similar across the proofs. while some simple lemmas and tactics where created
to handle the most tedious elements these did not make effective use of
hypotheses from the context. Improved proof search for permutations is
required to simplify such developments.

