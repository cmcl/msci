\section{Formalisation}

The formalisation was performed using the Coq proof
assisstant~\cite{Coq:manual}. In the spirit of the POPLMARK
challenge~\cite{Aydemir:2005:MMM}, the development utilises available
libraries and infrastructure as much as possible. In particular,
\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM} developed a metatheory
library for handling, among other things, association lists (environments) and
variable representation. The current development uses the version of the
library distributed by~\citeauthor{Park:2014:MMW}~\cite{Park:2014:MMW} which
has been updated for Coq version 8.4pl4. One goal of the formalisation effort
was to minimise the extent to which the development differs from the
pen-and-paper presentation and avoid questions of adequacy of the
representation. Some changes were made to ease the mechanisation and are
highlighted where they occur.

\subsection{Preliminaries}\label{sec:approach}

The locally nameless cofinite quantification (LNCQ) representation for binders
and free variables was adopted for both GV and CP, following
\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM}. This first-order
variant prevents variable-capture issues when compared with other first-order
approaches. Alternatively, I could have chosen a higher-order abstract syntax
(HOAS) approach such as the parametric version (PHOAS) described
by~\citeauthor{Chlipala:2008:PHOAS}~\cite{Chlipala:2008:PHOAS}. PHOAS relies
on the metalanguage (Coq, in this case) for handling binders and substitution
so has less overhead in managing syntax than the locally nameless
approach. However, it is less clear how to handle linearity using this
approach and has the disadvantage of being able to represent terms not valid
in the language being mechanised (so called ``exotic'' terms). Further, a
higher-order approach would prevent the use of the Metatheory library which
provides a number of useful tactics for manipulating typing contexts.

Previous efforts to mechanise \fpop~\cite{Park:2014:MMW} illustrate how to
handle a language containing both linear and non-linear contexts. However, to
stay true to CP/GV, I maintain a single context. Interestingly, it became
clear that the mechanised version of CP required explicit weakening and
contraction to facilitate the reduction rules (\S~\ref{sec:cp}).

For handling binder freshness, I chose to adopt the cofinite quantification
approach described by~\citeauthor{Aydemir:2008:EFM}, which excludes a finite
set of variables from being considered as the binder. In contrast, the
traditional ``exists-fresh'' approach, where the binder is only required to be
fresh within the abstraction's body, does not produce strong enough induction
principles in some cases, e.g. weakening~\cite{Aydemir:2008:EFM}. This
decision impacts upon the premises of the CP/GV typing rules (\S~\ref{sec:cp}
and \S~\ref{sec:gv}), however these changes do not alter the typing of
processes nor the semantics of the translation from GV to CP. An equivalence
between ``exists-fresh'' and cofinite quantification definitions of the
simply-typed $\lambda$-calculus is given by
\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM}, and I suspect a similar
result could be derived straightforwardly (but tediously) for the CP/GV
system.

Another aspect of the encoding is how to define terms. The intrinsic encoding
described by~\citeauthor{Benton:2012:STT}~\cite{Benton:2012:STT} is one
approach, indexing terms by their type so as to prevent ill-formed terms from
being constructed. However, there are issues with using this encoding for
CP/GV: $(1)$ it is not immediately clear how to handle linear contexts using
the de Bruijn variable encoding, since in the intrinsic setting the
environment automatically supports weakening; $(2)$ an intrinsic encoding of
GV terms would be complicated by the need to enforce that session types occur
in certain instances (by use of a predicate); and $(3)$ well-typed terms
require extra assumptions about binder freshness which cannot reasonably be
expressed as a function type. For these reasons, an intrinsic approach does
not offer much benefit since a well-typed term relation would still need to be
defined. Therefore, I chose an extrinsic encoding for terms and define
well-typed terms as a separate inductive type.

\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM} note that the size of
their language proof infrastructure is proportional to the number of binding
constructs. In the case of a simply typed lambda calculus this is not onerous,
but GV has four binding constructs, CP has six and propositions have
two. Indeed, a lot of effort was expended setting up this
infrastructure. However, I believe one of the contributions of this work is to
articulate what one needs from state of the art metatheory libraries in order
to handle session-based languages, and more generally, linear type systems.

\citeauthor{Park:2014:MMW} describe a technique for removing non-linear
contexts from typing judgements. While their work extended to \fpop it is not
clear how to handle an environment containing both non-linear and linear types
as in CP/GV. I wish to maintain as close a relationship as possible to the
pen-and-paper system presented by \citeauthor{Wadler:2014}, so separating out
the non-linear and linear components (as in \fpop) is not an option at this
stage.

\begin{comment}
\section{Issues}

\begin{itemize}
\item What is required of a Metatheory library for linear type systems?
  Permutation reasoning? Weakening/Contraction for non-linear components?
\item Discuss the different approaches that were taken to maintain close
  correspondence with the paper presentation
\item Make sure to note that the paper leaves details (a lot of the ``cruft'')
  out of the proofs e.g. where weakening is applied in the typing derivations
\item Mention the use of Metatheory and its limitations when mechanising
  linear type systems like GV/CP
\end{itemize}
\end{comment}

\subsection{The process calculus CP}\label{sec:cp}

\input{cp}

\begin{comment}
FIGURES

propositions

processes

equivalences and axiomatic cut
\end{comment}

Formalising the process calculus CP provided many challenges for the
metatheory library since to my knowledge, the library has never been applied
to a process calculus before. So for instance, in the lambda calculus one
typically does not work up to permutation of binders, whereas in a process
calculus these kinds of structural equivalences are standard. Additionally,
substitution and opening are only defined for names. In other words, one
cannot substitute an arbitrary process term for a name, unlike in the lambda
calculus. Thus frameworks which assume a lambda calculi style operations are
not immediately amenable to the CP setting~\cite{Lee:2012}. Ordering in
environments is ignored so the development required lemmas allowing the
permutation of environments. Support for permutating environments is not
currently supported by the Metatheory library but some general lemmas were
added as a result of this work. Additionally, custom tactics were developed to
automatatically solve some simple permutation goals, however these tactics are
not a ``catch all'' and some generalising still needs to be done.

% Description of the typing rules...

Figure~\ref{fig:cll} presents the process calculus CP in LNCQ form (ignoring
permutation premises). In other words bound variables are represented using de
Brujin indices and free variables are represented using an abstract type
provided by the Metatheory library. The rules with binders reflect this by
specifying the type of a bound variable in the constructor rather than the
name. Cofinite quantification is performed in the hypotheses to these rules by
excluding a finite set of names from opening the subprocesses. Note that the
rules for polymorphism, $\exists$ and $\forall$, were included in the final
Coq development since GV (\S~\ref{sec:gv}) does not support polymorphism
making the rules unnecessary for the translation. They appear in the Figure
simply for the sake of completeness. Additionally, as noted by
\citeauthor{Lee:2012}, support for polymorphism in type variables creates an
explosion of the infrastructure requiring duplication of opening and
substituting definitions and lemmas.

% ...leading on to the discussion of the explicit weakening...

Initially, an implicit notion of weakening was chosen so as to be in line with
\citeauthor{Wadler:2014}'s definitions. However, explicit weakening is
required to differentiate the principal cut elimination rules for the various
interactions between servers and clients. contraction is not supported because
of the difficulty in specifying its principal cut elimination rule. In order
to specify the rule one requires to show the following:

\todo{v A -> !0(A).P | Q{x/x'}
  ==>
v A -> !0(A).P | v A -> !0(A).P | contract (Gamma) Q}

Unfortunately, the reduction relation does not have knowledge of the
environment typing process P. This fact does not impact upon expressing
weakening since for weakening only the names (not the bindings) within the
environment are required. The paper presentation does not specify where
weakening could occur; it is tacitly assumed weakening can always be pushed
further up the derivation tree e.g. AxCut rule. a consequence of this choice
is that for the empty choice rule there may only be a single binding in the
context (cf. an arbitrary context in \citeauthor{Wadler:2014}'s
presentation~\cite{Wadler:2014}).

% ...leading on to presentation of principal cut elimination rules...

\input{principalcp}

The principal cut elimination rules are presented in Figure~\ref{??}. These
rules are expressed to indicate some additional complexity not present in the
original presentation. For example, structural equivalences and associativity
are not unique and different process terms result depending on initial binder
permutation. ${0 <~> 1}P$ indicates the permuting of binders $0$ and $1$ in
process P. The paper presentation specifies only one possible associativity
rule lending to the specific split of the linear environments. The reduction
rules were complicated to prove due to being composed of multiple constructor
instances and differed significantly to the standard operational semantics one
typically deals with in lambda calculus formalisms.
linear environments caused a problem with the splitting up of variables.

% ...commuting conversions

\input{commconvonecp}

\input{commconvtwocp}

The commuting conversions are shown in Figures~\ref{??} and~\ref{??}. Notice
that permutating binders is essential to formulating the majority of these
rules.

% ...leading on to a discussion of further CP-specific issues encountered.

\subsection{The functional language GV}\label{sec:gv}

\begin{comment}
FIGURES

types

terms
\end{comment}

\input{gv}

The typing judgements for the functional language GV are shown in
Figure~\ref{??}. Note that I have altered the $n$-ary branch and choice
constructs to the restricted binary versions in order to avoid tricky
formulations. The change is not restrictive however, since CP itself provides
only binary versions of plus ($\oplus$) and with ($\with$) constructs.

On a more technical note, all types are amalgamated into one inductive
definition and provide a predicate for restricting typing rules to consider
only valid session types. \citeauthor{Wadler:2014} defines GV types as a set
of mutually recursive definitions; session types are types which may contain
types as subcomponents (arguments to send, for example). Unfortunately,
handling mutually inductive definitions in Coq can be quite involved;
requiring one to either rely on the Coq system to provide a stronger mutual
induction principle or defining one manually. Such a definition is possible,
but it complicates applications of the induction principle.

Another issue related to types is how to representing type kinds (linearity or
non-linearity). Initially the type definition was defined as an indexed
family, indexed by the kind. While this definition has the advantage of
preventing certain ill-formed types, such as a non-linear session type, it
complicated later definitions and proofs. For instance, the decidability of
equality on types now relied on an axiom permitting heterogeneous
equality~\cite{??}. Generally, it is better to avoid unnecessary postulates if
possible. Further, when formulating the typing judgements the kind must be
given to each type hypothesised and most rules (e.g. ...) are polymorphic in
the kind, motivating an awkward inspection function to be applied per
application of a constructor \todo{clean this description up}. A solution to
this was to define a well-formed type relation and do away with the type
dependency. these issues highlighted that one should be careful and
considerate regarding when to use dependent types. these issues highlighted
that one should be careful and considerate how the representation will affect
later expressiveness.

explicit limiting and unlimiting of abstraction to ensure rules are syntax
directed - permits inductive argument on terms when translating from gv to cp

metatheory library and the library of tactics from software foundations are
used extensively: metatheory does not handle multiple binders (check this?)
maybe add a generic tactic to handle multiple binders... or perhaps add a
lemmas for handling binders

\subsection{CPS translation GV to CP}

type inference of gv terms required cluttered definition of translation with
impossible error cases. avoided a dependently typed solution due to
difficulties encountered earlier in the development. james mckinna et
al. suggest encoding the successful cases in an inductive type and then
defining the function to act on these cases. will this really work? this means
a graph of positive cases for types then a function definition for it too

translation uses the coq-ext-lib from michela for monadic notation

Currently the formalisation does not handle this. Future work.
