\section{Formalisation}

The formalisation was performed using the Coq proof
assistant~\cite{Coq:manual}. In the spirit of the POPLMARK
challenge~\cite{Aydemir:2005:MMM}, the development utilises available
libraries and infrastructure as much as possible. In particular,
\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM} developed a metatheory
library for handling, among other things, association lists (environments) and
variable representation. The current development uses the version of the
library distributed by~\citeauthor{Park:2014:MMW}~\cite{Park:2014:MMW} which
has been updated for Coq version 8.4pl4. One goal of the formalisation effort
was to minimise the extent to which the development differs from the
pen-and-paper presentation and avoid questions of adequacy of the
representation. Some changes were made to ease the mechanisation and are
highlighted where they occur.

\subsection{Preliminaries}\label{sec:approach}

The locally nameless cofinite quantification (LNCQ) representation for binders
and free variables was adopted for both GV and CP, following
\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM}. This first-order
variant prevents variable-capture issues when compared with other first-order
approaches. Alternatively, I could have chosen a higher-order abstract syntax
(HOAS) approach such as the parametric version (PHOAS) described
by~\citeauthor{Chlipala:2008:PHOAS}~\cite{Chlipala:2008:PHOAS}. PHOAS relies
on the metalanguage (Coq, in this case) for handling binders and substitution
so has less overhead in managing syntax than the locally nameless
approach. However, it is less clear how to handle linearity using this
approach and has the disadvantage of being able to represent terms not valid
in the language being mechanised (so called ``exotic'' terms). Further, a
higher-order approach would prevent the use of the Metatheory library which
provides a number of useful tactics for manipulating typing contexts.

Previous efforts to mechanise \fpop~\cite{Park:2014:MMW} illustrate how to
handle a language containing both linear and non-linear contexts. However, to
stay true to CP/GV, I maintain a single context. Interestingly, it became
clear that the mechanised version of CP required explicit weakening and
contraction to facilitate the reduction rules (\S~\ref{sec:cp}).

For handling binder freshness, I chose to adopt the cofinite quantification
approach described by~\citeauthor{Aydemir:2008:EFM}, which excludes a finite
set of variables from being considered as the binder. In contrast, the
traditional ``exists-fresh'' approach, where the binder is only required to be
fresh within the abstraction's body, does not produce strong enough induction
principles in some cases, e.g. weakening~\cite{Aydemir:2008:EFM}. This
decision impacts upon the premises of the CP/GV typing rules (\S~\ref{sec:cp}
and \S~\ref{sec:gv}), however these changes do not alter the typing of
processes nor the semantics of the translation from GV to CP. An equivalence
between ``exists-fresh'' and cofinite quantification definitions for the
simply-typed $\lambda$-calculus ($\stlc$) is given by
\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM}, and I suspect a similar
result could be derived straightforwardly (but tediously) for the CP/GV
system.

Another aspect of the encoding is how to define terms. The intrinsic encoding
described by~\citeauthor{Benton:2012:STT}~\cite{Benton:2012:STT} is one
approach, indexing terms by their type so as to prevent ill-formed terms from
being constructed. However, there are issues with using this encoding for
CP/GV:
\begin{enumerate}
\item It is not immediately clear how to handle linear contexts using the de
  Bruijn variable encoding, since in the intrinsic setting the environment
  automatically supports weakening
\item An intrinsic encoding of GV terms would be complicated by the need to
  enforce that session types occur in certain instances (by use of a
  predicate)
\item Well-typed terms require extra assumptions about binder freshness which
  cannot reasonably be expressed as a function type.
\end{enumerate}

For these reasons, an intrinsic approach does not offer much benefit since a
well-typed term relation would still need to be defined. Therefore, I chose an
extrinsic encoding for terms and define well-typed terms as a separate
inductive type.

\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM} note that the size of
their language proof infrastructure is proportional to the number of binding
constructs. In the case of a $\stlc$ this is not onerous, but GV has four
binding constructs, CP has six and propositions have two. Indeed, a lot of
effort was expended setting up this infrastructure. However, I believe one of
the contributions of this work is to articulate what one needs from state of
the art metatheory libraries in order to handle session-based languages, and
more generally, linear type systems.

\citeauthor{Park:2014:MMW} describe a technique for removing non-linear
contexts from typing judgements. While their work extended to \fpop it is not
clear how to handle an environment containing both non-linear and linear types
as in CP/GV. I wish to maintain as close a relationship as possible to the
pen-and-paper system presented by \citeauthor{Wadler:2014}, so separating out
the non-linear and linear components (as in \fpop) is not an option at this
stage.

\begin{comment}
\section{Issues}

\begin{itemize}
\item What is required of a Metatheory library for linear type systems?
  Permutation reasoning? Weakening/Contraction for non-linear components?
\item Discuss the different approaches that were taken to maintain close
  correspondence with the paper presentation
\item Make sure to note that the paper leaves details (a lot of the ``cruft'')
  out of the proofs e.g. where weakening is applied in the typing derivations
\item Mention the use of Metatheory and its limitations when mechanising
  linear type systems like GV/CP
\end{itemize}
\end{comment}

\subsection{The process calculus CP}\label{sec:cp}

\input{cp}

\begin{comment}
FIGURES

equivalences and axiomatic cut
\end{comment}

Figure~\ref{fig:cll} presents the well-typed process relation for CP as an
inductive type in Coq. Each typing rule in CP is represented as a constructor
of the inductive type \coqe$cp_rule$. The inductive type is indexed by a
process (\coqe$proc$) and an environment (\coqe$penv$) indicated by them
appearing after the colon on the first line. \coqe$forall$ quantifies over
hypotheses in a constructor; everything before the comma. Compound hypotheses
are given labels, such as \key{PER}, which is the default name used to
introduce them into the proof context during theorem proving. Some details
such as environment uniqueness have been elided, but notice the use of LNCQ
form within the hypotheses of the rules with binders. These rules exclude a
finite set of names \coqe$L$ from opening the subprocesses where opening a
process \coqe$P$ with name \coqe$x$ is denoted by \coqe$open_proc P x$. Bound
variables are represented using de Bruijn indices and free variables are
represented using an abstract type, \coqe$atom$, provided by the Metatheory
library. The rules with binders specify the type of a bound variable in the
constructor rather than a name. Note that the rules for polymorphism,
$\exists$ and $\forall$, were not included in the final Coq development since
GV (\S~\ref{sec:gv}) does not support polymorphism making the rules
unnecessary for the translation. Further, as noted by \citeauthor{Lee:2012},
support for polymorphism in type variables creates an explosion of the
infrastructure requiring additional of opening and substituting definitions
and lemmas.

Formalising the process calculus CP provided many challenges for the
Metatheory library since to my knowledge, the library has never been applied
to a process calculus before, and the treatment of names, binding and
substitution differ to that present in $\lambda$-calculi. For instance, in the
$\stlc$ one typically does not work up to permutation of binders, whereas in a
process calculus these kinds of structural equivalences are
standard. Additionally, substitution and opening are only defined for
names. In other words, one cannot substitute an arbitrary process term for a
name, unlike in the $\lambda$-calculus. Thus, frameworks which assume
$\lambda$-calculi style operations are not immediately amenable to the CP
setting~\cite{Lee:2012}.

Ordering in environments is ignored so the development required operations for
handling permutations of environments as shown in the \coqe$cp_rule$
definition. The Metatheory library does not currently support permutating
environments, however some basic lemmas were added as a result of this work,
and custom tactics were developed to automatically solve certain permutation
goals via simple proof search. Even so, there are still many cases where
sequences of primitive lemmas, such as those handling append, commutativity
and transitivity, need to be applied manually. A lot of these cases are
similar but there would not appear to be an advantage in developing a tactic
to do the work. Doing so would create a dependency between a hard-to-debug and
obscure Ltac definition and the proofs which utilise it, becoming a burden to
change. Further, such a development philosophy treats Ltac tactics akin to
macros, a shorthand for expressing an oft-repeated sequence of tactics, and it
is doubtful much can be gained from it unless the execution of Ltac tactics
can be made more transparent.

\subsubsection{Subject Reduction Theorem}

The subject reduction theorem can be expressed in Coq as follows:
\begin{coq}
Theorem proc_sub_red:
  forall $\Gamma$ P Q
         (WT: P $\tpvdash_{cp}$ $\Gamma$)
         (RED: P $\becomes$ Q),
    Q $\tpvdash_{cp}$ $\Gamma$.
Proof. ii; gen $\Gamma$; induction RED; subst; eauto. Qed.
\end{coq}

\coqe$Theorem$ or \coqe$Lemma$ indicate the start of a proposition to be
proved (Coq makes no distinction between the two keywords). The statement to
prove is that if \coqe$P$ is a well-typed process in a context $\Gamma$ and
\coqe$P$ reduces to \coqe$Q$ then \coqe$Q$ is well-typed in $\Gamma$. The
proof environment is started using \coqe$Proof.$ and then followed by a
sequence of tactics until all goals have been proven. After execution of
\coqe$Qed.$ the Coq system inspects the validity of the proof object
constructed by the tactics and, if valid, will add the theorem to the
environment.

Some tactics used above are shorthands for commonly combined tactics, for
example \coqe$ii$ repeatedly performs hypothesis introduction then
simplification until no more can be peformed, and \coqe$gen$ $\Gamma$
generalises the goal over the typing environment $\Gamma$.

\input{principalcp}

The \key{RED} hypothesis refers to an inductive type defining the principal
cut reductions and commuting conversions on processes. The inductive cases in
\coqe$proc_sub_red$ were complicated to prove due to the process terms being
composed of multiple constructor instances, differing significantly from the
standard operational semantics typically dealt with in $\lambda$-calculus
formalisms. To mitigate the complexity, the proof for each case was written as
a separate lemma (termed ``reduction lemmas''). The last step in the proof of
\coqe$proc_sub_red$, \coqe$eauto$, solves the goals generated by the induction
using the reduction lemmas.

An example of a reduction lemma is shown in Figure~\ref{fig:principal}. Cut of
server accept against client request involves three constructor instances, the
outer cut and the two dual constructors as the subprocesses. As shown, to
specify this compound process requires the explicit use of de Bruijn
indices. These indices will be replaced in the proof by a sufficiently fresh
name after destructing \key{WT} to obtain the derivations for \coqe$P$ and
\coqe$Q$. Most proofs followed a pattern but it was difficult to automate
satisfactorily for similar reasons to those stated for automating permutation
manipulation. However, some tasks were delegated to custom tactics, for
example instantiating cofinitely quantified hypotheses with a newly introduced
fresh variable.

Initially, an implicit notion of weakening was chosen so as to be in line with
\citeauthor{Wadler:2014}'s definitions. However, explicit weakening is
required to differentiate the principal cut elimination rules for the
interactions between servers and clients (spawning and weakening). The paper
presentation does not specify where weakening could occur; it is tacitly
assumed weakening can always be pushed further up the derivation tree to a
base rule e.g. \coqe$cp_fwd$ rule. Contraction is not supported in the
formalisation due to the difficulty in specifying its principal cut
elimination rule. In order to specify the rule one requires to show the
following:
\begin{gather*}
\tm{\nu A.(!0(A).P \mid ?z[[A]].Q)}
\quad \becomes \quad \\
\tm{\nu A.(!0(A).P \mid \nu A.(!0(A).P \mid \key{contract}_{\Gamma} Q))}
\end{gather*}

where $\tm{?z[[A]].Q}$ denotes contraction by $z$ of two channels of type $A$
in $Q$, $\tm{P^y} \tpvdash \tp{{?\Gamma} \comma \tmof{y}A}$ for some
sufficiently fresh name $y$ and $\key{contract}_{\Gamma} \tm{Q}$ contracts
$\tm{Q}$ with the names in $\Gamma$.

Unfortunately, the reduction relation does not have knowledge of the
environment, $\Gamma$, typing process $\tm{P}$ because this information is
only available after the compound cut is destructed, yielding a derivation for
$\tm{P}$. This fact does not impact upon expressing weakening because only the
names (not the types in the bindings) within the environment are required for
weakening.

In the empty choice rule there may only be a single binding in the context
(cf. an arbitrary context in \citeauthor{Wadler:2014}'s
presentation~\cite{Wadler:2014}). \todo{look into a fix for the empty choice
  rule} While this diverges with the presentation of linear logic it became
necessary to do this because otherwise there would be no relationship between
the free variables within a process and its environment. no there is a clear
iff relationship which permits reduction rules to apply such as i think
weakening but it should be any rule which takes some fvs of proc and infers
something about environments from that...otherwise what it is not at all clear
how to encode the reductions in a relation, the subprocesses environments are
created from destructing the environment

% ...commuting conversions

%\input{cpstructeq}

%\input{commconvonecp}

%\input{commconvtwocp}

The structural equivalences and commuting conversions are shown in
Figure~\ref{??} and Figures~\ref{??} and~\ref{??}, respectively. These rules
are expressed to indicate some additional complexity not present in the
original presentation. For example, structural equivalences are not unique and
different process terms result depending on initial binder
permutation. $\permb{0}{1}$ indicates the permuting of binders $0$ and $1$ in
process $\tm{P}$. The paper presentation specifies only one possible
associativity rule lending to the specific split of the linear environments.
Notice that permutating binders is essential to formulating the majority of
these rules. Analysing permutations containing singleton bindings was
necessary for the commuting conversion rules. Singleton bindings can occur
frequently during a proof and it is sometimes necessary to analyse a
permutation of the form P++x~A ~~ Q++R to determine whether x occurs in Q or
R. the rule for associativity requires this and so do quite a few reduction
rules. since this involves some element of searching the context followed by
destructing hypotheses it could be effectively automated in permutations in
subprocesses which use a binder of the parent. the rule for associativity w
linear environments caused a problem with the splitting up of variables.

% ... subject reduction ...

subject reduction proof proceeds by induction on the reduction relation then
is automatic by appeal to the proved well-typed reductions.

% ... cut elimination ....

\subsubsection{Top-level Cut Elimination}

The theorem for top-level cut elimination can be expressed as:
\begin{coq}
Theorem proc_cut_elim:
  forall P $\Gamma$
         (WT: P $\tpcp$ $\Gamma$),
    exists Q, P $\becomes^\star$ Q /\ $\sim$ is_cut Q.
\end{coq}

where $\becomes^\star$ denotes the reflexive and transitive closure of
$\becomes$ and $\sim$ \coqe$is_cut Q$ expresses that Q is not a cut.

Unfortunately, the theorem has eluded attempts at mechanised proof so far. The
hypothesis of a well-typed process \coqe$P$ is not sufficient to prove the
theorem. For instance, the \coqe$cp_cut$ constructor does not ensure that
there is a duality relationship between the subprocesses. Following informal
proofs, it is likely to require a termination measure involving the length of
the derivation is needed. Additionally, the locally nameless representation
creates subgoals for which it is not clear how to prove. For instance, a
commuting conversion is only defined if the logical operator corresponds to a
free variable but induction on \key{WT} yields two cases one of which applies
the logical operator to the top-level cut variable. Since there is no
relationship between the subprocesses, in general it will not be possible to
apply a principal cut rule.

\subsection{The functional language GV}\label{sec:gv}

%\input{gv}

The typing judgements for the functional language GV are shown in
Figure~\ref{??}. Note that I have altered the $n$-ary branch and choice
constructs to the restricted binary versions in order to avoid tricky
formulations. The change is not restrictive however, since CP itself provides
only binary versions of plus ($\oplus$) and with ($\with$)
constructs. Explicit limiting and unlimiting of abstraction was chosen to
enable the inclusion of these rules in the translation from GV to CP, which is
performed by induction on GV terms.

On a more technical note, all types are amalgamated into one inductive
definition and provide a predicate for restricting typing rules to consider
only valid session types. \citeauthor{Wadler:2014} defines GV types as a set
of mutually recursive definitions; session types are types which may contain
types as subcomponents (arguments to send, for example). Unfortunately,
handling mutually inductive definitions in Coq can be quite involved;
requiring one to either rely on the Coq system to provide a stronger mutual
induction principle or defining one manually. Such a definition is possible,
but it complicates applications of the induction principle.

Another issue related to types is how to representing type kinds (linearity or
non-linearity). Initially the type definition was defined as an indexed
family, indexed by the kind. While this definition has the advantage of
preventing certain ill-formed types, such as a non-linear session type, it
complicated later definitions and proofs. For instance, the decidability of
equality on types now relied on an axiom permitting heterogeneous
equality~\cite{??}. Generally, it is better to avoid unnecessary postulates if
possible. Further, when formulating the typing judgements the kind must be
given to each type hypothesised and most rules (e.g. ...) are polymorphic in
the kind, motivating an awkward inspection function to be applied per
application of a constructor \todo{clean this description up}. A solution to
this was to define a well-formed type relation and do away with the type
dependency. these issues highlighted that one should be careful and
considerate regarding when to use dependent types. these issues highlighted
that one should be careful and considerate how the representation will affect
later expressiveness.

similar infrastructure for gv required as for cp copy and paste job

\subsection{CPS translation GV to CP}

require type inference on gv terms to construct corresponding cp terms but we
are assured that this is okay since the theorem only provides a well-typed gv
term in a context that is well-defined

type inference of gv terms required cluttered definition of translation with
impossible error cases. avoided a dependently typed solution due to
difficulties encountered earlier in the development. james mckinna et
al. suggest encoding the successful cases in an inductive type and then
defining the function to act on these cases. will this really work? this means
a graph of positive cases for types then a function definition for it too this
seems rather pathetic because i have to define th einference on terms and it
seems wrong i have then to handle environments but also nonenvironments i
can't remember what they are called unbound variables that is it they have to
be handled as well so we have all this clutter and the types too that is the
important part but the environments then have to be handled linearly
unfortunately very bad because we could just use the well typed term relation
for most of this headachey breaky stuff i'm not sure about that but what is
can i say so for example lets see see no i also have to thing about the
general guidelines i can be giving for this section so we have something to
say about guidelines do we yes of course we have can say that out of this we
realised that it is important for a representation to be good enough to
succintly express that which we wish to say in particular the graph relation
the inductive type that we define propositionally is better one would think
than the other way the monadic possible failure way is not producing a valid
thing that we can inductively induct on so we concern ourself with the proof
of translation preserving the typed term and i think it is important to
remember the problem we had with the inductive hypothesis we had something do
with translating the inner term of a weakening term in a larger context than
the one we had available and that was to do with the destruction of the term
inference thing i think no no it was something else we had the inner term and
we had it well typed in an environment that was smaller sure and then our
thing our function was saying we had some environment that typed both the
inner term and the outer weakening term but we know that is wrong so wrong so
here is perhaps where the inductive type graph relation thing wins out in that
we can have a hypothesis for the inner term which is sufficiently small for
the environment and we pattern match not only on the term but also on the
environment yes that is we will do because it seems to work only when we get
to the messy two the messy two constructor caases will this become a bigg
problem inthat we will then have all this stuff about environment being split
and needing to be recombined in the composite process that is a term not a
process but the point still stands it will need to be about the term and the
splitting is really a duplication of the unfortunately the well typed term
relation but otherwise i cannot think how to reuse or obtain that information
for the translation to cp

Translation uses the coq-ext-lib from michela for monadic notation

Currently the formalisation does not handle this. Future work.

\todo{how might we make progress on this?}

\subsection{Issues and Guidelines}

modularised development; tactics slow prover took a long time to reduce proofs
so split the structure of the development \todo{picture of development
  structure}.

tactic performance is an issue to be addressed due to the slow proof search a
future work (things like deciding equations on finite sets)

One of the design goals of the development was to re-use existing libraries
wherever possible. two such instances proved extremely useful. first is the
Metatheory library developed at upenn. second, also from upenn, is the library
of tactics from software foundations.

software foundations helped in many ways: simplifying tacticals, ease of
introducing proved assertions, etc. limited use of names introduced by prover
leading to more robust development.

used extensively: metatheory does not handle multiple binders (check this?)
maybe add a generic tactic to handle multiple binders... or perhaps add a
lemmas for handling binders

Linearity create issues with typing some examples; the splitting of the
environment was non-deterministic so required side-conditions to enforce a
certain splitting of binders. For example the rule for associativity of
cut. \todo{how to handle? is there a better way?}

Base rules and rules with binders required permutation assumptions to allow
any ordering of the constituent contexts. a lot of the proofs for well-typed
reductions involved small amounts of permuting environments which were largely
similar across the proofs. while some simple lemmas and tactics where created
to handle the most tedious elements these did not make effective use of
hypotheses from the context. Improved proof search for permutations is
required to simplify such developments.

