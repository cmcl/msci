\section{Formalisation}

The formalisation was performed using the Coq proof
assistant~\cite{Coq:manual}. In the spirit of the POPLMARK
challenge~\cite{Aydemir:2005:MMM}, the development utilises available
libraries and infrastructure as much as possible. In particular,
\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM} developed a metatheory
library for handling, among other things, association lists (environments) and
variable representation. The current development uses the version of the
library distributed by~\citeauthor{Park:2014:MMW}~\cite{Park:2014:MMW} which
has been updated for Coq version 8.4pl4. One goal of the formalisation effort
was to minimise the extent to which the development differs from the
pen-and-paper presentation and avoid questions of adequacy of the
representation. Some changes were made to ease the mechanisation and are
highlighted where they occur.

\subsection{Preliminaries}\label{sec:approach}

The locally nameless cofinite quantification (LNCQ) representation for binders
and free variables was adopted for both GV and CP, following
\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM}. This first-order
variant prevents variable-capture issues when compared with other first-order
approaches. Alternatively, I could have chosen a higher-order abstract syntax
(HOAS) approach such as the parametric version (PHOAS) described
by~\citeauthor{Chlipala:2008:PHOAS}~\cite{Chlipala:2008:PHOAS}. PHOAS relies
on the metalanguage (Coq, in this case) for handling binders and substitution
so has less overhead in managing syntax than the locally nameless
approach. However, it is less clear how to handle linearity using this
approach and has the disadvantage of being able to represent terms not valid
in the language being mechanised (so called ``exotic'' terms). Further, a
higher-order approach would prevent the use of the Metatheory library which
provides a number of useful tactics for manipulating typing contexts.

Previous efforts to mechanise \fpop~\cite{Park:2014:MMW} illustrate how to
handle a language containing both linear and non-linear contexts. However, to
stay true to CP/GV, I maintain a single context. Interestingly, it became
clear that the mechanised version of CP required explicit weakening and
contraction to facilitate the reduction rules (\S~\ref{sec:cp}).

For handling binder freshness, I chose to adopt the cofinite quantification
approach described by~\citeauthor{Aydemir:2008:EFM}, which excludes a finite
set of variables from being considered as the binder. In contrast, the
traditional ``exists-fresh'' approach, where the binder is only required to be
fresh within the abstraction's body, does not produce strong enough induction
principles in some cases, e.g. weakening~\cite{Aydemir:2008:EFM}. This
decision impacts upon the premises of the CP/GV typing rules (\S~\ref{sec:cp}
and \S~\ref{sec:gv}), however these changes do not alter the typing of
processes nor the semantics of the translation from GV to CP. An equivalence
between ``exists-fresh'' and cofinite quantification definitions for the
simply-typed $\lambda$-calculus ($\stlc$) is given by
\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM}, and I suspect a similar
result could be derived straightforwardly (but tediously) for the CP/GV
system.

Another aspect of the encoding is how to define terms. The intrinsic encoding
described by~\citeauthor{Benton:2012:STT}~\cite{Benton:2012:STT} is one
approach, indexing terms by their type so as to prevent ill-formed terms from
being constructed. However, there are issues with using this encoding for
CP/GV:
\begin{enumerate}
\item It is not immediately clear how to handle linear contexts using the de
  Bruijn variable encoding, since in the intrinsic setting the environment
  automatically supports weakening
\item An intrinsic encoding of GV terms would be complicated by the need to
  enforce that session types occur in certain instances (by use of a
  predicate)
\item Well-typed terms require extra assumptions about binder freshness which
  cannot reasonably be expressed as a function type.
\end{enumerate}

For these reasons, an intrinsic approach does not offer much benefit since a
well-typed term relation would still need to be defined. Therefore, I chose an
extrinsic encoding for terms and define well-typed terms as a separate
inductive type.

\citeauthor{Aydemir:2008:EFM}~\cite{Aydemir:2008:EFM} note that the size of
their language proof infrastructure is proportional to the number of binding
constructs. In the case of a $\stlc$ this is not onerous, but GV has four
binding constructs, CP has six and propositions have two. Indeed, a lot of
effort was expended setting up this infrastructure. However, I believe one of
the contributions of this work is to articulate what one needs from state of
the art metatheory libraries in order to handle session-based languages, and
more generally, linear type systems.

\citeauthor{Park:2014:MMW} describe a technique for removing non-linear
contexts from typing judgements. While their work extended to \fpop it is not
clear how to handle an environment containing both non-linear and linear types
as in CP/GV. I wish to maintain as close a relationship as possible to the
pen-and-paper system presented by \citeauthor{Wadler:2014}, so separating out
the non-linear and linear components (as in \fpop) is not an option at this
stage.

\begin{comment}
\section{Issues}

\begin{itemize}
\item What is required of a Metatheory library for linear type systems?
  Permutation reasoning? Weakening/Contraction for non-linear components?
\item Discuss the different approaches that were taken to maintain close
  correspondence with the paper presentation
\item Make sure to note that the paper leaves details (a lot of the ``cruft'')
  out of the proofs e.g. where weakening is applied in the typing derivations
\item Mention the use of Metatheory and its limitations when mechanising
  linear type systems like GV/CP
\end{itemize}
\end{comment}

\subsection{The process calculus CP}\label{sec:cp}

\input{cp}
\input{papercut}

Figure~\ref{fig:cll} presents the well-typed process relation for CP as an
inductive type in Coq. Each typing rule in CP is represented as a constructor
of the inductive type \coqe$cp_rule$. The inductive type is indexed by a
process (\coqe$proc$) and an environment (\coqe$penv$) indicated by them
appearing after the colon on the first line. \coqe$forall$ quantifies over
hypotheses in a constructor; everything before the comma. Compound hypotheses
are given labels, such as \key{PER}, which is the default name used to
introduce them into the proof context during theorem proving. Some details
such as environment uniqueness have been elided, but notice the use of LNCQ
form within the hypotheses of the rules with binders. These rules exclude a
finite set of names \coqe$L$ from opening the subprocesses where opening a
process \coqe$P$ with name \coqe$x$ is denoted by \coqe$open_proc P x$ and
corresponds to replacing de Bruijn index 0 with \coqe!x!. Bound variables are
represented using de Bruijn indices and free variables are represented using
an abstract type, \coqe$atom$, provided by the Metatheory library. The rules
with binders specify the type of a bound variable in the constructor rather
than a name.

Note that the rules for polymorphism, $\exists$ and $\forall$, were not
included in the final Coq development since GV (\S~\ref{sec:gv}) does not
support polymorphism making the rules unnecessary for the
translation. Further, as noted by \citeauthor{Lee:2012}, support for
polymorphism in type variables creates an explosion of the infrastructure
requiring additional open and substitution definitions and related lemmata.

Formalising the process calculus CP provided many challenges for the
Metatheory library since to my knowledge, the library has never been applied
to a process calculus before, and the treatment of names, binding and
substitution differ to that present in $\lambda$-calculi. For instance, in the
$\stlc$ one typically does not work up to permutation of binders, whereas in a
process calculus these kinds of structural equivalences are
standard. Additionally, substitution and opening are only defined for
names. In other words, one cannot substitute an arbitrary process term for a
name, unlike in the $\lambda$-calculus. Thus, frameworks which assume
$\lambda$-calculi style operations are not immediately amenable to the CP
setting~\cite{Lee:2012}.

Ordering in environments is ignored so the development required operations for
handling permutations of environments as shown in the \coqe$cp_rule$
definition. The Metatheory library does not currently support permutating
environments, however some basic lemmata were added as a result of this work,
and custom tactics were developed to automatically solve certain permutation
goals via simple proof search. Even so, there are still many cases where
sequences of primitive lemmata, such as those handling append, commutativity
and transitivity, need to be applied manually. A lot of these cases are
similar but there would not appear to be an advantage in developing a tactic
to do the work. Doing so would create a dependency between a hard-to-debug and
obscure Ltac definition and the proofs which utilise it, becoming a burden to
change. Further, such a development philosophy treats Ltac tactics akin to
macros, a shorthand for expressing an oft-repeated sequence of tactics, and it
is doubtful much can be gained from it unless the execution of Ltac tactics
can be made more transparent.

As an illustrative example of the differences between the Coq encoding of the
rules and the paper presentation consider the pen-and-paper version of the cut
rule shown in Figure~\ref{fig:papercut}. There is no overhead when introducing
binders to the subprocesses or for environment ordering, but apart from these
representation issues the rules look similar which could assist the uptake of
such formal tools by programming language researchers.

\subsubsection{Subject Reduction Theorem}

The subject reduction theorem can be expressed in Coq as follows:
\begin{coq}
Theorem proc_sub_red:
  forall $\Gamma$ P Q
         (WT: P $\tpvdash_{cp}$ $\Gamma$)
         (RED: P $\becomes$ Q),
    Q $\tpvdash_{cp}$ $\Gamma$.
Proof. ii; gen $\Gamma$; induction RED; subst; eauto. Qed.
\end{coq}

\coqe$Theorem$ or \coqe$Lemma$ indicate the start of a proposition to be
proved (Coq makes no distinction between the two keywords). The statement to
prove is that if \coqe$P$ is a well-typed process in a context $\Gamma$ and
\coqe$P$ reduces to \coqe$Q$ then \coqe$Q$ is well-typed in $\Gamma$. The
proof environment is started using \coqe$Proof.$ and then followed by a
sequence of tactics until all goals have been proven. After execution of
\coqe$Qed.$ the Coq system inspects the validity of the proof object
constructed by the tactics and, if valid, will add the theorem to the
environment.

Some tactics used above are shorthands for commonly combined tactics, for
example \coqe$ii$ repeatedly performs hypothesis introduction then
simplification until no more can be peformed, and \coqe$gen$ $\Gamma$
generalises the goal over the typing environment $\Gamma$.

\input{principalcp}

The \key{RED} hypothesis refers to an inductive type defining the principal
cut reductions and commuting conversions on processes. The inductive cases in
\coqe$proc_sub_red$ were complicated to prove due to the process terms being
composed of multiple constructor instances, differing significantly from the
standard operational semantics typically dealt with in $\lambda$-calculus
formalisms. To mitigate the complexity, the proof for each case was written as
a separate lemma (termed ``reduction lemmata''). The last step in the proof of
\coqe$proc_sub_red$, \coqe$eauto$, solves the goals generated by the induction
using the reduction lemmata.

An example of a reduction lemma, and its counterpart in
\citeauthor{Wadler:2014}'s presentation, is shown in
Figure~\ref{fig:principal}. While \citeauthor{Wadler:2014} describes the cut
reductions using derivation trees, this structure is left implicit in the Coq
encoding. Cut of server accept against client request involves three
constructor instances, the outer cut and the two dual constructors as the
subprocesses. As shown, to specify this compound process requires the explicit
use of de Bruijn indices. These indices will be replaced in the proof by a
sufficiently fresh name after destructing \key{WT} to obtain the derivations
for \coqe!P! and \coqe!Q!. Most proofs followed a pattern but it was difficult
to automate satisfactorily for similar reasons to those stated for automating
permutation manipulation. However, some tasks were delegated to custom
tactics, for example instantiating cofinitely quantified hypotheses with a
newly introduced fresh variable.

\subsubsection{Issues for Reduction}

A consequence of defining the reduction relation with respect to compound
process terms is that the subprocesses have implicit derivations which are
unavailable in the conclusion. For example, \coqe$reduce_spawn$ provides no
information about the environment which types \coqe$P$ in the statement of the
lemma. The derivation for \coqe$P$ (similarly, \coqe$Q$) is obtained by
inverting the hypothesis \key{WT} and specialising the resulting assumptions
to a newly introduced fresh variable (to replace the de Bruijn index). In the
case of \coqe$reduce_spawn$, two inversions are required to get derivations
for \coqe$P$ and \coqe$Q$. This property of the reduction relation affects the
definitions of the typing rules for CP, though it was not obvious at the time.
Notable examples are the structural rules, weakening and contraction, and
empty choice.

\textbf{Weakening and Contraction.} Initially, an implicit notion of weakening
was chosen so as to be in line with \citeauthor{Wadler:2014}'s
definitions. However, explicit weakening is required to differentiate the
principal cut elimination rules for the interactions between servers and
clients; spawning and garbage collection (weakening). The paper presentation
does not specify where weakening could occur due to its implicit nature; it is
tacitly assumed weakening can always be pushed further up the derivation tree
to a base rule e.g. \coqe$cp_fwd$ rule. The same is true of contraction where
it is defined as a substitution of names on process terms, not a
syntax-directed rule.

\begin{samepage}
Contraction is not supported in the formalisation due to the difficulty in
specifying its principal cut elimination rule.  In order to specify the rule
one requires knowledge of the environment typing process \coqe$P$ with the
rule of the form:
\begin{gather*}
\tm{\nu A.(!0(A).P \mid ?z[[A]].Q)}
\quad \becomes \quad \\
\tm{\nu A.(!0(A).P \mid \nu A.(!0(A).P \mid \key{contract}_{\Gamma} Q))}
\end{gather*}

where $\Gamma$ is the assumed environment typing \coqe$P$, $\tm{?z[[A]].Q}$
denotes contraction by $z$ of two channels of type $A$ in $Q$ and
$\key{contract}_{\Gamma} \tm{Q}$ contracts $\tm{Q}$ with the bindings in
$\Gamma$. Unfortunately, as explained above, there is no way for the reduction
relation to obtain the environment $\Gamma$.
\end{samepage}

The reader may wonder how weakening is supported if not
contraction. Fortunately, weakening only requires the names (not the types in
the bindings) within the environment, and these may be obtained by relying on
the following lemma relating the free variables of a process and the domain of
its environment:
\begin{coq}
Lemma in_env_fv:
  forall P $\Gamma$ x (WT: P $\tpcp$ $\Gamma$),
    x $\in$ dom $\Gamma$ $\iff$ x $\in$ fv_proc P.
\end{coq}

\textbf{Empty Choice.} The development provides a restricted form of the empty
choice rule. In contrast to the arbitrary context in
\citeauthor{Wadler:2014}'s presentation~\cite{Wadler:2014}, \coqe!cp_empcho!
may only have a single binding in the context. Adding an arbitrary environment
to the empty choice constructor would prohibit \coqe$in_env_fv$ since its
process no longer contains all the names of its environment. Not only would
weakening be inadmissible if \coqe$in_env_fv$ did not hold, but also axiom cut
reduction:
\begin{coq}
Lemma reduce_axcut:
  forall P A (w : atom) $\Gamma$
         (NFV: w $\notin$ fv_proc P)
         (WT: $\nu$ A.(w $\link$ 0 $\mid$ P) $\tpcp$ $\Gamma$),
    (open_proc P w) $\tpcp$ $\Gamma$.
\end{coq}

The proof of \coqe$reduce_axcut$ needs to rename the variable chosen to open
the process \coqe$P$ in the conclusion. Initially, the variable is a fresh
\coqe$x$ introduced during the proof, but using the following
\coqe$typing_rename$ lemma, \coqe$w$ can replace \coqe$x$ by appeal to
\coqe$in_env_fv$:
\begin{coq}
Lemma typing_rename:
  forall $\Gamma$ P k (x y : atom) A
         (NINX: x $\notin$ dom $\Gamma$ $\cup$ fv_proc P)
         (NINY: y $\notin$ dom $\Gamma$ $\cup$ fv_proc P)
         (WTX: $\open{k}{x}{P}$ $\tpcp$ (x $\sim$ A) ++ $\Gamma$),
    $\open{k}{y}{P}$ $\tpcp$ (y $\sim$ A) ++ $\Gamma$.
\end{coq}

where $\open{k}{x}{P}$ denotes replacing de Bruijn index $k$ with free
variable $x$ in process $P$~\footnote{For example, \coqe!$\open{0}{x}{P}
  \defeq$ open_proc P x!}.

While this diverges with the linear logic rule for top, I believe this variant
of CP still admits a valid translation from GV. In a private communication,
Philip Wadler echoed these remarks but pointed out that this is likely because
the GV-to-CP translation does not require the full expressiveness of CP.

\subsubsection{Equivalences and Conversions}

The structural equivalences and commuting conversions have some additional
complexity not present in the original presentation. For instance, structural
equivalences are not unique and different process terms result depending on
the initial binder permutation. Consider the associativity rule for which the
paper presentation specifies only one rule:
\begin{coq}
Lemma assoc_1:
  forall P Q R A B $\Gamma$
         (LCP: forall x, lc_proc (open_proc P x))
         (WT: $\nu$ B.($\nu$ A.(P $\mid$ Q) $\mid$ R) $\tpcp$ $\Gamma$),
    $\nu$ A.(P $\mid$ $\nu$ B.($\permb{0}{1}{Q}$ $\mid$ R)) $\tpcp$ $\Gamma$.
\end{coq}

where \coqe$lc_proc P$ specifies that \coqe$P$ is a \textit{local closure}, in
other words contains no unbound de Bruijn indices and $\permb{0}{1}Q$
indicates the permuting of binders 0 and 1 in process $\tm{Q}$. The typing
rule for \coqe$cp_cut$ does not specify any particular split of the linear
environment in the conclusion; splitting is non-deterministic. So, in order to
prove the above statement, it must be true in \key{WT} that \coqe$Q$ takes the
binder of type \coqe$B$ and not \coqe$P$, hence the local closure assumption
on \coqe$open_proc P x$. Similarly, to prove the converse statement
\coqe$forall x, open_proc R x$ must be assumed. For commuting conversions,
permutating binders is essential in formulating the majority of the rules.

\input{extract}

Regarding commuting conversions, many require analysing permutations
containing singleton bindings. Consider the following the Coq encoding of the
commuting conversion for output (cf. Figure~\ref{fig:ccoutput}):
\begin{coq}
Lemma reduce_cc_multi_one:
  forall P Q R (x:atom) A B $\Gamma$ (LCQ: lc_proc Q)
         (WT: $\nu$ A.([B] x.(P $\mid$ Q) $\mid$ R) $\tpcp$ $\Gamma$),
    [B] x.($\nu$ A.($\permb{0}{1}{P}$ $\mid$ R) $\mid$ Q) $\tpcp$ $\Gamma$.
\end{coq}

Notice again, as in the case for \coqe!assoc_1!, it necessary to stipulate
that \coqe!Q! is locally closed to ensure \coqe!P! receives the channel
created by the outer cut as part of its environment.

After inverting \key{WT} and specialising the resulting hypotheses, regarding
the subprocesses, with fresh variables the proof state is as follows:

\begin{tabular}{l}
\begin{coq}
1 subgoal
(* ... other elements of the proof context ... *)
CPR : $\open{0}{y}{R}~\tpcp$ y$\sim$A++$\GD$Q
CPQ : Q $\tpcp$ x$\sim$C++$\GD$Q0
CPP : $\open{0}{z}{\open{1}{y}{P}}$ $\tpcp$ z$\sim$B++$\GD$P0
NIN : y $\notin$ {x} $\cup$ dom $\GD$Q0
PER : Permutation $\Gamma$ ($\GD$P++$\GD$Q)
PER0 : Permutation ($\GD$P++y$\sim$A) (x$\sim$B$\otimes$C++$\GD$P0++$\GD$Q0)
\end{coq}\\ \hline
\begin{coq}
x(B).($\nu$ A.($\open{0}{1}{P}$ $\mid$ Q)) $\tpcp$ $\Gamma$
\end{coq}
\end{tabular}
\\~\\

In order to prove the goal, $\Gamma$ must be replaced with the sublists typing
\coqe!P!, \coqe!Q! and \coqe!R!. Therefore, \coqe!PER0! needs to be modified
such that the left-hand side is $\GD$P and the right-hand side has
\coqe!y$\sim$A!  removed. In other words, the binding of \coqe!y! within
\coqe!$\GD$P0! must be extracted (by \coqe!NIN!, \coqe!y! must be in
\coqe!$\GD$P0!). Since many of the commuting conversion rules required this
sort of reasoning, separating the work out into a tactic appears
beneficial. Indeed, Ltac is well-suited for this type of work, searching the
context and manipulating hypotheses of a particular form. The Ltac script in
Figure~\ref{fig:extract} does the job and after executing
\coqe!extract_bnd y A!, \key{PER0} becomes:
\begin{coq}
Permutation ($\GD$P++y$\sim$A) (x$\sim$B$\otimes$C++E1++y$\sim$A++E2++$\GD$Q0)
\end{coq}
where E1 and E2 are sublists of $\GD$P0.

However, the task of removing the binding from the permutation is not
automated to keep information loss within the context transparent to an
individual proof. The tactic is more modular and general in this form,
permitting composition and permutation of tactic sequences.

\subsubsection{Top-level Cut Elimination}\label{sec:cutelim}

\begin{figure}
\begin{coq}
Theorem proc_cut_elim:
  forall P $\Gamma$
         (WT: P $\tpcp$ $\Gamma$),
    exists Q, P $\becomes^\star$ Q /\ $\sim$ is_cut Q.
\end{coq}
\caption{Cut elimination theorem}
\label{fig:cutelim}
\end{figure}

The theorem for top-level cut elimination appears in Figure~\ref{fig:cutelim}
where $\becomes^\star$ denotes the reflexive and transitive closure of
$\becomes$ and $\sim$ \coqe$is_cut Q$ expresses that Q is not a cut.
Unfortunately, the theorem has eluded attempts at mechanised proof so far. The
hypothesis of a well-typed process \coqe$P$ is not sufficient to prove the
theorem. For instance, the \coqe$cp_cut$ constructor requires a
``relatedness'' condition on the subprocesses to ensure that they eventually
operate dually on the bound variable. The condition is required after
induction on \key{WT} yields a goal similar to the following:
\begin{coq}
exists R, ($\nu$A.([B]0.($P_1$ $\mid$ $P_2$) $\mid$ Q)) $\becomes^\star$ R
          /\ (is_cut R -> False)
\end{coq}

The left subprocess has been destructed to a logical operator which operates
on the bound variable of the cut, but since there is no relationship between
the two subprocesses it is impossible to prove the goal.

Following informal proofs, it seems likely that the proof requires a
termination measure involving the length of the derivation. In hindsight,
however, I do not believe the current encoding is the best for this sort of
reasoning. Formalised efforts for proving cut elimination for other logics
have adopted an approach which permits reasoning on the structure of
derivations represented explicitly as
trees~\cite{Tews:2013,Dawson:2010,Dawson:2002}. Whereas, in the approach taken
here, that information is limited and ephemeral in that it is only available
during the proving of a derivation e.g. a reduction lemma. It is future work
to investigate these and other strategies which will assist in formalising
cut-elimination for CP.

\subsection{The functional language GV}\label{sec:gv}

\input{gv}
\input{letscompare}

The typing judgements for the functional language GV are shown in
Figure~\ref{fig:gv}. Note that I have altered the $n$-ary branch and choice
constructs to the restricted binary versions in order to avoid tricky
formulations. The change is not restrictive however, since CP itself provides
only binary versions of plus ($\oplus$) and with ($\with$)
constructs. Explicit limiting and unlimiting of abstraction was chosen to
enable the inclusion of these rules in the translation from GV to CP, which is
performed by induction on GV terms and is therefore
syntax-directed. Figure~\ref{fig:letscompare} presents the informal version of
the well-typed let rule. While \citeauthor{Char:2012} suggests using
two-indices for handling multiple binders~\cite{Char:2012}, such a strategy
seems excessive for the GV let rule and the only rule to benefit from multiple
binders. For that reason, the simple solution of performing two openings was
chosen.

As shown in \S~\ref{sec:fusion}, \citeauthor{Wadler:2014} defines GV types as
a set of mutually recursive definitions. Unfortunately, handling mutually
inductive definitions in Coq can be quite involved, requiring one to either
rely on the Coq system to provide a stronger mutual induction principle or
define one manually. Such a definition is possible, but it complicates
applications of the induction principle. Therefore, all types are amalgamated
into one inductive definition.

\input{wft}

Another issue related to types is how to represent type kinds, linear and
unlimited kind. Initially the type definition was indexed by the kind and
while this definition has the advantage of preventing certain ill-formed
types, such as a non-linear session type, it complicated later definitions and
proofs. For instance, the decidability of equality on types now relied on an
axiom permitting heterogeneous equality~\cite{McBride:1999}. Generally, it is
better to avoid unnecessary postulates if possible. Further, when formulating
the typing rules, the kind must be given to each type hypothesised and most
rules are polymorphic in the kind, requring an awkward kind inspection
function to be applied to each type within a binding. The alternative approach
taken was to define a well-formed type relation and remove the kind index from
the type definition. Figure~\ref{fig:wft} shows the well-formedness for the
linear function type. Notice, that even though the conclusion must have linear
kind, as in rule \coqe!wt_tm_labs!, the argument and result types can have any
kind, highlighting the aforementioned kind polymorphism. An additional
predicate was defined to ensure typing rules consider only valid session
types. The predicate appears in the well-formedness constructors for session
types, ensuring the continuation is a session type and not, for example, a
linear function type. These issues highlight that one should be mindful of the
chosen representation and its effect on later expressiveness.

\subsection{CPS translation from GV to CP}\label{sec:trans}

Following the original work by \citeauthor{Wadler:2014}, the translation from
GV to CP requires defining translations on types and terms of GV.

\subsubsection{Types}

\input{gv-trans-types}

The translation of from GV types to CP propositons is shown in
Figure~\ref{fig:typtrans}. Translation of session types is noteworthy for
taking a GV session type to its notional dual in CP, reflecting the semantic
difference between channels in GV and those in CP. For GV, the type refers to
the action on the channel endpoint. For CP, the proposition refers to the
\textit{interface} exposed to a user of the channel. For example, output in GV
corresponds to input in CP; the action on the channel endpoint is to output a
value, and to do this the channel must input the value.

\subsubsection{Terms}

\input{gv-cps-cp}

The translation of GV terms to CP processes is given, following
\citeauthor{Wadler:2014}, using continuation-passing style. In this style a
well-typed term in GV is parameterised by a name representing the channel
which communicates the result of the translation (a CP process). Due to the
locally nameless representation, the translation of GV terms required a
procedure to infer the type of terms in order to specify the binding type for
the corresponding CP process. An example, translating choice, is shown in
Figure~\ref{fig:trans}, unfortunately this definition is wrong because the
process P is not being opened and therefore during the proof we are unable to
reason about the typability of P opened with a sufficiently fresh variable the
reason is that P is a subprocess and therefore is not locally closed note that
I am using \citeauthor{Wadler:2014}'s style but using an LNCQ notation for
clarity. In order to construct the cut and output processes, the type of
\tm{M} and \tm{N} must be computed. So, the translation takes the environment,
in which the term is typed, as an argument to query the type of free
variables. Unfortunately, this query returns a type of \coqe!option typ!
(\coqe!option! is equivalent to \tm{Maybe} in Haskell) since it may not be
found, but in fact since only well-typed GV terms are translated it is
guaranteed to return something!  Therefore, the definition is unnecessarily
cluttered with impossible error cases.

The term translation was initially designed using a monadic approach to
account for the possible error cases in retreiving names from the
environment. The coq-ext-lib
library~\footnote{https://github.com/coq-ext-lib/coq-ext-lib} provides
definitions for monad, functor and applicative type classes and associated
instances for basic Coq types. It is similar in style to Haskell's support for
these objects. Unfortunately, this definition did not yield a suitable
function to prove the translation well-typed because the function needs to be
able to operate structurally on not only terms, but also the environment and
the processes being constructed. Consider again the translation of case. here
we require to consider nl and nr in a separate environment than m and type the
resulting case term in the combined environment we also need to open these
terms using cofinite quantification to yield a suitable induction
principle. all told the function is not readily definable. further problems do
arise as we shall see.

An alternative formulation, which avoids the need to handle error cases, is to
first define only the positive cases using an inductive type, the so-called
\textit{graph relation}, and then define a dependent function that only
considers the positive cases for which the relation is
defined~\cite{McKinna:2009}. However, several issues were encountered when
attempting to specify the translation. The graph relation requires much of the
clutter present in both the \coqe!cp_rule! and \coqe!wt_tm! definition such as
linear enviornments and cofinite quantification. the proof of cps translation
using the graph relation is a work in progress. the main difficulty is that
the representation requires opening both GV terms and CP processes in
hypotheses which would appear to duplicate a lot of the overhead of their
respective rule definitions.  again, representation is important even if the
proof has still eluded us.  while it is remains to be seen whether or not the
graph relation assists us in these matters it is nonetheless worth trying to
express a property in multiple ways

Roughly, the theorm aiming to prove is:
\begin{coq}
Theorem cps_trans_wt:
  forall $\Phi$ M T z P $\Gamma$ $\GD$
         (WT: $\Phi$ $\tpgv$ M$\of{T}$)
         (NIN: z $\notin$ GVFV M $\cup$ dom $\Phi$)
         (ENV: trans_env $\Phi$ = $\GD$)
         (PER: Permutation $\Gamma$ ($\GD$ ++ z $\sim$ $\sem{T}$t))
         (CP: transTm M $\Phi$ z P),
    P $\tpcp$ $\Gamma$.
\end{coq}

\subsection{Issues and Guidelines}

\textbf{Modularised development.} Not only is modular development an important
programming practice in general, but in the context of theorem proving it has
the advantage of reducing interactive load time. Executing proof scripts can
be time-consuming especially if those scripts make frequent use of decision
procedures performing proof search, then the execution time is proportional to
the size of the proof context and complexity of the goal. There were cases in
which the system crashed unexpectedly during interactive mode and it is
conceivable that having many large proofs in-memory could be a contributing
factor. For these reasons, the development is partitioned into GV and CP
sections each further sub-divided into definition, infrastructure and typing
modules (CP also has an additional module for reduction lemmata). The CPS
translation is also defined separately. Load time is considerably improved and
the consequences of a crash are mitigated.

\textbf{Representation.} As discussed in the previous sections representation
choice is a central issue when formalising programming language
metatheory. Decisions on how to define, for instance, types, calculi and
translations impact on future definitions and properties dependent on
them. Considerations range from how much expressive power provided by the
theorem prover to exploit to the choice of encoding to use for the properties
of interest. Some of these decisions may impact the correspondence between the
formal and informal systems e.g. the empty choice rule and omission of
contraction in CP, and the omission of n-ary branch and choice from GV, and
such things are necessarily a tradeoff between simplicity and
adequacy.

Interplay between different decisions is an additional factor to consider. For
example, the representation choice for linearity makes environment splitting
non-deterministic. Applying inversion on well-typed process terms requires, in
some cases such as \coqe!assoc_1!, to enforce a certain split between
subprocesses. The choice of LNCQ representation forces a local closure
assumption in these cases. Alternative representations for the CP calculus,
such as separating the rules (link, cut, etc.) from the derivations
(associativity, reductions, etc.)~\cite{Tews:2013,Dawson:2010,Dawson:2002},
may ameliorate the handling of linearity by allowing one to explicate the
particular splitting for a derivation.

\textbf{Library reuse.} One of the design goals of the development was to
reuse existing libraries wherever possible. Two such instances proved
extremely useful, first is the Metatheory library as already
mentioned. Second, is the library of tactics provided by the Software
Foundations online book~\cite{SF}. The tactics helped in many ways including
simplifying the introduction of assertions and providing convenient shorthands
for commonly executed sequences of tactics. An example for simplifying
assertions would be the \coqe!forwards! tactic permitting lemma
instantiation. Consider the following example (during the proof of $assoc_1$):
\begin{tabular}{l}
\begin{coq}
1 subgoal
(* ... other elements of the proof context ... *)
PER : Permutation (x $\sim$ B ++ $\GD$P) ($\GD$P0++$\GD$Q0)
UN : uniq (x $\sim$ B ++ $\GD$P)
\end{coq}\\ \hline
\begin{coq}
$\nu$ A.(P $\mid$ $\nu$ B.($\open{0}{1}{Q}$ $\mid$ R)) $\tpcp$ $\Gamma$
\end{coq}
\end{tabular}
\\~\\

In order to proceed with proving the goal, it is necessary to show that
$\GD$P0++$\GD$Q0 is unique. The proof follows from \key{PER} and \key{UN} upon
application of an appropriate lemma, and there are two possible ways to prove
this using plain Coq. One way is to apply the lemma to one of \key{PER} or
\key{UN} thereby replacing the chosen hypothesis with the conclusion of the
lemma (termed \textit{forward reasoning}). The opposite of forward reasoning,
namely backward reasoning, does not apply since the goal has nothing to do
uniqueness of $\GD$P0++$\GD$Q0. However, using the \coqe!assert! tactic one
can utilise backward reasoning. An assertion temporarily focuses a different
goal (the asserted goal) which may then be tackled using tactics in the
standard way. Unfortunately, this approach requires the explicit writing of
the goal, namely \coqe!uniq ($\GD$P0++$\GD$Q0)!, into the proof script of the
lemma cluttering the proof script sensitive to change because \coqe!$\GD$P0!
and \coqe!$\GD$Q0! are ephemeral names introduced by prover.

The \coqe!forwards! tactic avoids the problem of explicit names by allowing
the uniqueness property to be proven succiently:
\begin{coq}
forwards UN1: uniq_perm PER UN.
\end{coq}

where \coqe!uniq_perm! is the appropriate lemma and \coqe!UN1! is the name
given to the desired uniqueness property. Note that \key{PER} and \key{UN} are
mentioned in the proof with \coqe!assert! so the use of \coqe!forwards! has
reduced the dependencies on introduced names. The improvement is more
pronounced in cases where the goal is particularly long involving many
variables from the context.

\textbf{Effective Automation.} It is worth considering situations where
automation would simplify proofs. Automation is already an important
consideration in the interactive theorem proving community. Examples of
automation from the current development have been highlighted, but particular
emphasis should be placed on tactics involving non-trivial proof search of the
context because these tend to be more widely applicable.
